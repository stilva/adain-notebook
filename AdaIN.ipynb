{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print('GPU::', tf.config.list_physical_devices('GPU'))\n",
    "print('version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "VGG_TENSOR_RC = IMAGE_SIZE//8 # input of 224 into vgg_19 outputs 28x28x512 @ block4_conv1\n",
    "THUMB_SIZE = IMAGE_SIZE//4\n",
    "batch_size = 18 #18 for enc & dec, 36 for enc only, 30 for dec only\n",
    "DATASET_LENGTH = 70000 // batch_size # style: 81445, content: 82612 # content face: 70000\n",
    "number_of_epochs = 20\n",
    "\n",
    "CHECKPOINT_DIR = './042.ckpts'\n",
    "LOGS_DIR = './logs/adain.encoder.decoder.v417'\n",
    "\n",
    "total_steps = DATASET_LENGTH * number_of_epochs\n",
    "\n",
    "test_sample = max(5, batch_size)\n",
    "\n",
    "layer_names = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    ")\n",
    "\n",
    "vgg_model_internal = tf.keras.Model(vgg.input, [vgg.get_layer(name).output for name in layer_names])\n",
    "vgg_model_internal.trainable = False\n",
    "vgg_model_internal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all_image_paths = ['../data/train2014/' + f for f in os.listdir('../data/train2014')]\n",
    "all_image_paths = [f for f in glob.glob('../ffhq-dataset/images224x224/**/*.png')]\n",
    "all_style_image_paths = [f for f in glob.glob('../data/wikiart/**/*')]\n",
    "\n",
    "def preprocess_image_from_path(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.cast(tf.image.decode_image(img, channels=3, expand_animations = False), tf.float32)\n",
    "\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    if shape[0] < shape[1]:\n",
    "        img = tf.image.resize(img, (IMAGE_SIZE, 10000), preserve_aspect_ratio=True)\n",
    "    elif shape[0] > shape[1]:\n",
    "        img = tf.image.resize(img, (10000, IMAGE_SIZE), preserve_aspect_ratio=True)\n",
    "    \n",
    "    if shape[0] >= IMAGE_SIZE and shape[1] >= IMAGE_SIZE:\n",
    "        img = tf.image.random_crop(img, (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    else:\n",
    "        img = tf.image.resize_with_pad(img, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    \n",
    "    return img / 255. # normalize to [0,1] range\n",
    "\n",
    "def preprocess_content_image(img):\n",
    "    \n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    \n",
    "    img = tf.image.random_hue(img, .1)\n",
    "    img = tf.image.random_brightness(img, .1)\n",
    "    img = tf.image.random_contrast(img, .85, 1.)\n",
    "    img = tf.image.random_saturation(img, .85, 1.)\n",
    "    \n",
    "    return img\n",
    "\n",
    "style_dataset = tf.data.Dataset.from_tensor_slices(all_style_image_paths)\n",
    "content_dataset = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "\n",
    "style_dataset = style_dataset.map(preprocess_image_from_path, num_parallel_calls=AUTOTUNE)\n",
    "style_dataset = style_dataset.shuffle(buffer_size=50).repeat()\n",
    "\n",
    "content_dataset = content_dataset.map(preprocess_image_from_path, num_parallel_calls=AUTOTUNE)\n",
    "content_dataset = content_dataset.map(preprocess_content_image, num_parallel_calls=AUTOTUNE)\n",
    "content_dataset = content_dataset.shuffle(buffer_size=50).repeat()\n",
    "\n",
    "dataset = tf.data.Dataset.zip((style_dataset, content_dataset))\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "base_style = [data for data in list(style_dataset.take(test_sample).as_numpy_iterator())]\n",
    "base_content = [data for data in list(content_dataset.take(test_sample).as_numpy_iterator())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is based on multiple open source code I've seen.\n",
    "# Probably https://github.com/jonrei/tf-AdaIN/blob/master/AdaIN.py\n",
    "class AdaIN(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AdaIN, self).__init__(**kwargs)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, _input):\n",
    "        style_features, content_features, alpha = _input\n",
    "\n",
    "        style_mean, style_variance = tf.nn.moments(style_features, [1,2], keepdims=True)\n",
    "        content_mean, content_variance = tf.nn.moments(content_features, [1,2], keepdims=True)\n",
    "\n",
    "        epsilon = 1e-5\n",
    "        \n",
    "        #batch_normalization breaks on tflite's Android GPU delegate\n",
    "        content_std = tf.math.sqrt(content_variance + epsilon)\n",
    "        style_std = tf.math.sqrt(style_variance + epsilon)\n",
    "\n",
    "        normalized_content_features = (content_features - content_mean) / (content_std + epsilon) * style_std + style_mean\n",
    "        \n",
    "        return alpha * normalized_content_features + (1 - alpha) * content_features\n",
    "\n",
    "# grabbed this from somewhere..., to avoid checkerboard pattern in output\n",
    "class ReflectionPad(layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [layers.InputSpec(ndim=4)]\n",
    "        super(ReflectionPad, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] + 2 * self.padding[0], input_shape[2] + 2 * self.padding[1], input_shape[3])\n",
    "    \n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        return tf.pad(input_tensor, [[0,0], [padding_height, padding_height], [padding_width, padding_width], [0,0] ], 'REFLECT')\n",
    "\n",
    "def get_style_loss(encoded_style, encoded_y):\n",
    "    loss = 0\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    for style, y in zip(encoded_style, encoded_y):\n",
    "        mean_style, variance_style = tf.nn.moments(style, [1,2], keepdims=True)\n",
    "        mean_y, variance_y = tf.nn.moments(y, [1,2], keepdims=True)\n",
    "        \n",
    "        std_style = tf.math.sqrt(variance_style + epsilon)\n",
    "        std_y = tf.math.sqrt(variance_y + epsilon)\n",
    "        \n",
    "        loss += tf.reduce_mean(tf.math.square(mean_style - mean_y))\n",
    "        loss += tf.reduce_mean(tf.math.square(std_style - std_y))\n",
    "        \n",
    "        # must have read something about log_cosh. Haven't tested on output quality diff between log_cosh & mse\n",
    "        loss += tf.reduce_mean(tf.reduce_sum(tf.reshape(keras.losses.log_cosh(mean_style, mean_y), (batch_size, -1)), axis=1))\n",
    "        loss += tf.reduce_mean(tf.reduce_sum(tf.reshape(keras.losses.log_cosh(std_style, std_y), (batch_size, -1)), axis=1))\n",
    "        \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e_loss_tracker = keras.metrics.Mean(name=\"encoder_loss\")\n",
    "d_loss_tracker = keras.metrics.Mean(name=\"decoder_loss\")\n",
    "\n",
    "adain_layer = AdaIN(name=\"AdaIN\", trainable=False)\n",
    "# issues with mediaPipe to pass a scalar\n",
    "const_alpha = tf.ones((batch_size, 1, 1, 512)) * .5\n",
    "\n",
    "checkpoint_manager = None\n",
    "\n",
    "def embed(container, image, i, j):\n",
    "    if len(image.shape) > 3:\n",
    "        batch, ii, ij, ic = image.shape\n",
    "    else:\n",
    "        batch = 1\n",
    "        ii, ij, ic = image.shape\n",
    "\n",
    "    for b in range(batch):\n",
    "        container[b, i:i + ii, j:j + ij] = image[b]\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, patience=0):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.tick = 0\n",
    "        self.writer = tf.summary.create_file_writer(LOGS_DIR)\n",
    "        self.e_train_loss = tf.keras.metrics.Mean('encoder_train_loss', dtype=tf.float32)\n",
    "        self.d_train_loss = tf.keras.metrics.Mean('decoder_train_loss', dtype=tf.float32)\n",
    "        self.learning_rate = tf.keras.metrics.Mean('learning_rate', dtype=tf.float32)\n",
    "        self.alpha = tf.ones((batch_size, 1, 1, 512)) * .5\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        net = tf.train.Checkpoint(\n",
    "            encoder=self.model.encoder,\n",
    "            decoder=self.model.decoder\n",
    "        )\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(1),\n",
    "            net=net,            \n",
    "            e_optimizer=self.model.e_optimizer,\n",
    "            d_optimizer=self.model.d_optimizer,\n",
    "            encoder=self.model.encoder,\n",
    "            decoder=self.model.decoder\n",
    "        )\n",
    "        \n",
    "        self.manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=3)\n",
    "        ckpt.restore(self.manager.latest_checkpoint)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "        self.manager.save()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.manager.save()\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.tick += 1\n",
    "                \n",
    "        if self.tick % 100 == 0:\n",
    "            self.e_train_loss(logs.get('encoder_loss'))\n",
    "            self.d_train_loss(logs.get('decoder_loss'))\n",
    "            self.learning_rate(self.model.e_optimizer.lr)\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar('encoder_train_loss', self.e_train_loss.result(), step=self.tick)\n",
    "                tf.summary.scalar('decoder_train_loss', self.d_train_loss.result(), step=self.tick)\n",
    "                tf.summary.scalar('learning_rate', self.learning_rate.result(), step=self.tick)\n",
    "                \n",
    "        if self.tick == 1 or (self.tick % 250 == 0 and self.tick < 1000) or self.tick % 1000 == 0: \n",
    "            batched_bs = tf.slice(base_style, [0, 0, 0, 0], [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "            batched_bc = tf.slice(base_content, [0, 0, 0, 0], [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "\n",
    "            preprocessed_input_style = keras.applications.vgg19.preprocess_input(batched_bs * 255.)\n",
    "            b_style = vgg_model_internal(preprocessed_input_style, training=False)[-1]\n",
    "            b_content = self.model.encoder(batched_bc, training=False)\n",
    "            \n",
    "            output, _ = self.model.decoder([b_style, b_content, self.alpha], training=False)\n",
    "            \n",
    "            resized_bs = tf.image.resize(batched_bs, (THUMB_SIZE, THUMB_SIZE), preserve_aspect_ratio=True)\n",
    "            resized_bc = tf.image.resize(batched_bc, (THUMB_SIZE, THUMB_SIZE), preserve_aspect_ratio=True)\n",
    "            \n",
    "            output = output.numpy()\n",
    "            embed(output, resized_bs.numpy(), 0, 0)\n",
    "            embed(output, resized_bc.numpy(), IMAGE_SIZE - THUMB_SIZE, 0)\n",
    "\n",
    "            np.reshape(output, (-1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.image(\"5 outputs\", output, max_outputs=5, step=self.tick)\n",
    "            \n",
    "            self.manager.save()\n",
    "            \n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def compile(self, e_optimizer, d_optimizer):\n",
    "        super(CustomModel, self).compile()\n",
    "        self.e_optimizer = e_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        #TODO: need to find a better way for the training cycles\n",
    "        \n",
    "        # avoids vgg19.preprocess_input from mutating the cached image\n",
    "        _input_style = tf.identity(data[0])\n",
    "        _input_content = tf.identity(data[1])\n",
    "        \n",
    "        # input images are [0, 1]\n",
    "        preprocessed_input_style = keras.applications.vgg19.preprocess_input(_input_style * 255.)\n",
    "        encoded_style = vgg_model_internal(preprocessed_input_style, training=False)\n",
    "        y_style = encoded_style[-1]\n",
    "        \n",
    "        preprocessed_input_content = keras.applications.vgg19.preprocess_input(_input_content * 255.)\n",
    "        encoded_content = vgg_model_internal(preprocessed_input_content, training=False)\n",
    "\n",
    "        loss = 0\n",
    "        encoder_loss = 0\n",
    "        y_content = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # input images are [0, 1]\n",
    "            y_content = self.encoder(_input_content, training=True)\n",
    "            \n",
    "            encoder_loss = tf.reduce_mean(tf.math.square(y_content - encoded_content[-1]))\n",
    "    \n",
    "        trainable_vars = self.encoder.trainable_variables\n",
    "        grads = tape.gradient(encoder_loss, trainable_vars)\n",
    "        self.e_optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        if y_content is None:\n",
    "            y_content = self.encoder(_input_content, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # input images are [0, 1]\n",
    "            y, adaIn_output = self.decoder([y_style, y_content, const_alpha], training=True)\n",
    "            \n",
    "            # y is [0, 1]\n",
    "            preprocessed_y = keras.applications.vgg19.preprocess_input(y * 255)\n",
    "            encoded_y = vgg_model_internal(preprocessed_y, training=False)\n",
    "\n",
    "            loss = get_style_loss(encoded_style, encoded_y) * 1e2\n",
    "            loss += tf.reduce_mean(tf.math.square(adaIn_output - encoded_y[-1]))\n",
    "            loss += tf.image.total_variation(y) * .5\n",
    "    \n",
    "        trainable_vars = self.decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        d_loss_tracker.update_state(loss)\n",
    "        e_loss_tracker.update_state(encoder_loss)\n",
    "\n",
    "        return {\"encoder_loss\": e_loss_tracker.result(), \"decoder_loss\": d_loss_tracker.result()}\n",
    "        \n",
    "def conv(x, ch, name, strides=1):\n",
    "    x = ReflectionPad((1, 1), name=name + 'reflection')(x)\n",
    "    x = layers.SeparableConv2D(ch, (3, 3), strides=strides, name=name + 'sepconv', use_bias=True, kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "    x = tfa.layers.InstanceNormalization(axis=3, center=True, scale=True)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_decoder_model(x):\n",
    "    # 56, 56, 512\n",
    "    x = conv(x, 512, 'block_5_')\n",
    "    \n",
    "    # 112, 112, 512\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    \n",
    "    x = conv(x, 256, 'block_4a_')\n",
    "    x = conv(x, 256, 'block_4b_')\n",
    "    x = conv(x, 256, 'block_4c_')\n",
    "    x = conv(x, 256, 'block_4d_')\n",
    "    \n",
    "    # 224, 224, 256\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    \n",
    "    x = conv(x, 128, 'block_3a_')\n",
    "    x = conv(x, 128, 'block_3b_')\n",
    "    \n",
    "    x = conv(x, 64, 'block_2a_')\n",
    "    \n",
    "    # 448, 448, 64\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    \n",
    "    x = conv(x, 64, 'block_1b_')\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    x = ReflectionPad((1, 1), name='block_reflection')(x)\n",
    "    x = layers.Conv2D(3, (3, 3), name='block_conv', kernel_initializer=initializer)(x)\n",
    "    return tf.math.tanh(x) * .5 + .5\n",
    "\n",
    "def get_encoder_model(x):\n",
    "    x = tf.reverse(x, [3])\n",
    "    x = x * 255. - [103.939, 116.779, 123.68]\n",
    "    # 448, 448, 3\n",
    "    x = conv(x, 64, 'block1_conv1') # 448, 448, 64\n",
    "    x = conv(x, 64, 'block1_conv2') # 448, 448, 64\n",
    "    \n",
    "    x = conv(x, 128, 'block2_conv1', strides=2) #224, 224, 128\n",
    "    x = conv(x, 128, 'block2_conv2') #224, 224, 128\n",
    "    \n",
    "    x = conv(x, 256, 'block3_conv1', strides=2) #112, 112, 256\n",
    "    x = conv(x, 256, 'block3_conv2') #112, 112, 256\n",
    "    \n",
    "    x = conv(x, 512, 'block4_conv1', strides=2) #56, 56, 512\n",
    "    x = conv(x, 512, 'block4_conv2') #56, 56, 512\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_input = keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "x = _input\n",
    "x = get_encoder_model(x)\n",
    "\n",
    "encoder = keras.Model(_input, x)\n",
    "\n",
    "_input_style = keras.Input((VGG_TENSOR_RC, VGG_TENSOR_RC, 512))\n",
    "_input_content = keras.Input((VGG_TENSOR_RC, VGG_TENSOR_RC, 512))\n",
    "_input_alpha = keras.Input(shape=(1, 1, 512))\n",
    "\n",
    "adaIn_output = AdaIN(name=\"AdaIN\")([_input_style, _input_content, _input_alpha])\n",
    "output = get_decoder_model(adaIn_output)\n",
    "\n",
    "decoder = keras.Model([_input_style, _input_content, _input_alpha], [output, adaIn_output])\n",
    "\n",
    "wrapper_model = CustomModel(encoder=encoder, decoder=decoder)\n",
    "wrapper_model.compile(\n",
    "    e_optimizer=keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, amsgrad=True),\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, amsgrad=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "wrapper_model.encoder.summary()\n",
    "wrapper_model.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "net = tf.train.Checkpoint(\n",
    "    encoder=wrapper_model.encoder,\n",
    "    decoder=wrapper_model.decoder\n",
    ")\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    step=tf.Variable(1),\n",
    "    net=net,\n",
    "    encoder=wrapper_model.encoder,\n",
    "    decoder=wrapper_model.decoder\n",
    ")\n",
    "\n",
    "ckpt.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "\n",
    "batched_bs = tf.slice(base_style, [0, 0, 0, 0], [5, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "batched_bc = tf.slice(base_content, [0, 0, 0, 0], [5, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "\n",
    "preprocessed_input_style = keras.applications.vgg19.preprocess_input(batched_bs * 255.)\n",
    "encoded_style = vgg_model_internal(preprocessed_input_style, training=False)\n",
    "\n",
    "ys = encoded_style[-1]\n",
    "yc = wrapper_model.encoder(batched_bc)\n",
    "\n",
    "y, _ = wrapper_model.decoder([ys, yc, tf.ones((5, 1, 1, 512)) * .5])\n",
    "\n",
    "fig = plt.figure(figsize = (20, 20))\n",
    "fig.add_subplot(3,3,1)\n",
    "plt.imshow((batched_bs[4].numpy() * 255.).astype(np.uint8))\n",
    "\n",
    "fig.add_subplot(3,3,2)\n",
    "plt.imshow((batched_bc[4].numpy() * 255.).astype(np.uint8))\n",
    "\n",
    "fig.add_subplot(3,3,3)\n",
    "plt.imshow((y[4].numpy()).astype(np.uint8))\n",
    "plt.show()\n",
    "\n",
    "def get_loss():\n",
    "    _batched_bc = tf.slice(base_content, [0, 0, 0, 0], [1, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "\n",
    "    preprocessed_input_content = keras.applications.vgg19.preprocess_input(_batched_bc * 255.)\n",
    "    encoded_content = vgg_model_internal(preprocessed_input_content, training=False)\n",
    "\n",
    "    encoder_loss = 0\n",
    "\n",
    "    # input images are [0, 1]\n",
    "    y_content = wrapper_model.encoder(_batched_bc)\n",
    "    encoder_loss += tf.keras.losses.log_cosh(encoded_content[-1], y_content)\n",
    "        \n",
    "    return tf.reduce_mean(encoder_loss).numpy()\n",
    "\n",
    "print(get_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrapper_model.fit(\n",
    "    dataset,\n",
    "    {},\n",
    "    epochs=number_of_epochs,\n",
    "    steps_per_epoch=DATASET_LENGTH,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        CustomCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "_input = keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "_output = wrapper_model.encoder(_input)\n",
    "model = keras.Model(_input, _output)\n",
    "tf.saved_model.save(model, './adain_encoder')\n",
    "\n",
    "_input = keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "# 'RGB'->'BGR'\n",
    "y = tf.reverse(_input, [3])\n",
    "y = y * 255. - [103.939, 116.779, 123.68]\n",
    "_output = vgg_model_internal(y)\n",
    "model = keras.Model(_input, _output[-1])\n",
    "tf.saved_model.save(model, './adain_vgg')\n",
    "\n",
    "s_input = keras.Input((VGG_TENSOR_RC, VGG_TENSOR_RC, 512))\n",
    "c_input = keras.Input((VGG_TENSOR_RC, VGG_TENSOR_RC, 512))\n",
    "a_input = keras.Input((1, 1, 512))\n",
    "\n",
    "_output, _ = wrapper_model.decoder([s_input, c_input, a_input])\n",
    "\n",
    "model = keras.Model([s_input, c_input, a_input], _output)\n",
    "tf.saved_model.save(model, './adain_decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to web models uses the following CLI command\n",
    "\n",
    "`tensorflowjs_converter ./adain_encoder/ ./new_web_models/encoder`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
